BIBLIOGRAPHY

#################################################################################
# "Effective Large-Scale Online Influence Maximization", [Lagrée et al., 2017]  #
#################################################################################

## CONTEXT

Online influence maximization with persistence (OIMP)
- (One-shot) (Full-information) (Offline) IM: given a (an undirected) graph (V, W), and a nonnegative integer parameter B, find a subset S of V of maximum size B such that it maximizes F(S) = \sum_{x \in S} \sum_{y \in N(x)} W[x, y] (i.e. maximizes the "expected spread", that is, the expected number of activated nodes, where W[x, y] is the probability that y is activated knowing that x is activated), where N(x) is the set of neighbors of node x (i.e. the set of node y such that W[x, y] > 0). B represents the (maximal) budget allocated to the activation of nodes in S (e.g. giving away iPhones to each person in S).
- Online IM: Not all the weights in W are accessible: need for inference of the diffusion model while running the IM campaign
- IM with persistence: The IM process is iterated several times (independently) (spreading the same type of information each time), and previously activated nodes remain active, thus do not yield any reward for the next steps. But previously activated nodes can be re-selected if needed.
- Multi-armed bandits: A set A of arms, a learner
At each step (until horizon T)
	The environment sets a reward r_i for all arm a_i
	The learner selects an arm a_i
	The learner only observes reward r_i
Goal: to minimize regret: at step t, R_t = \sum_{k <= t} r_{a_k*}-r_{a_k} where a_{i*} is the best arm
Heuristic: Use Optimism In The Face Of Uncertainty, Exploration/exploitation dilemma
Algorithms: UCB (and its variants), Thompson sampling (and its extensions), ...

## CONTRIBUTIONS OF THE ARTICLE

Suggestion and description of a method solving (approximately) the OIMP problem that gives "high quality" results, is "magnitude faster" than state-of-the-art methods, and "makes almost no assumptions on the diffusion medium".

## STATE-OF-THE-ART & DISCUSSION

- First work: [Kempe et al., 2003]
Diffusion models: W = influence score: Linear Threshold (LT) and Independent Cascade (IC) and Weighted Cas-
cade (WC)
Greedy algorithms using sub-modularity (seen in class) and non-scalable
- Solving exact IM with diffusion model LT or IC: NP-hard
- Benchmark: see [Arora et al., 2017]
- Online approach: use of multi-armed bandits
- Issues with state-of-the-art: inference of diffusion model at small horizon (10-100 spreads), too simplistic diffusion meta-models
- "multi-armed bandit idea initially employed in" [Lei et al., 2015], but they infer "the diffusion edge
probabilities of a known graph"
- Use of Good-Turing estimator ([Bubeck et al., 2013])

## METHOD (called "GT-UCB")
- Inference of "the values of the candidates", not of the diffusion model
- Use of Good-Turing estimator
- Assumption: There exists a subset C of nodes (called spread-seed candidates, or candidates) such that the nodes in C are the only ones that can diffuse and activate other nodes. (Thus S is included in C) Each candidate has a support, that is, a subset of nodes having an unknown activation probability knowing the considered candidate is activated. Supports of distinct candidates can overlap. (for C = V, it is the classic IM problem)
- Update of the candidates values: use of "feedback" for each candidate k in C, that is, the set of activated nodes in the support of k. The reward of arm "selecting candidate k at step t" is the number of nodes in the support of k that have been activated at step t.
- Diminishing returns property ~ submodularity
- Missing mass of candidate k at step t Rk(t): expected number of nodes remaining to be activated in the support of k at step t
- Estimator of missing mass (using a variant of the Good-Turing estimator): 
Rk(t)^ = 1/nk(t) * sum_{u in Ak} Uk(u, t) * prod_{l \neq k} Zl(u, t)
where nk(t) = number of times arm k has been selected up to step t
Uk(u, t) = 1 iff. u has been activated exactly once by candidate k up to step t (hapax)
Zl(u, t) = 1 iff. u has never been activated by candidate k up to step td
- ALGORITHM input C, horizon T, B = 1 (one candidate selected at each step)
Initialization: play each candidate k once, nk(0) = 1, initialize Rk(0) for all k
for t from T-|C| to T do
	compute score Bk(t) for each candidate k
	choose ONE (B=1) arm k that maximizes Bk(t)
	observe reward and update cumulative reward
	update statistics used to compute bk(t)
return cumulative reward
where bk(t) = Rk(t)^ + (1+sqrt(2))*sqrt(lambda_k(t)*log(4*t)/nk(t))+log(4*t)/(3*nk(t))
where lambda_k(t) = sum_{1 <= s <= nk(t)} F(k)_s/nk(t) expected cumulative spread for candidate k at step t
- Theorem to deduce the upper bound from above
- Extension for B > 1 => select the B arms having the highest values of Bk(t)

EXPERIMENTS:
- C is the set of "centroids" nodes with respect to influence (having the less overlapping supports): methods MaxDegree, Greedy MaxCover, DivRank
- Baselines methods: Random, [Lei et al., 2015]

################################################################################################################################
# "Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality", [Bubeck et al., 2013] #
################################################################################################################################

TODO

#################################################################################
# "Getting Recommender Systems to Think Outside the Box" [Abbassi et al., 2009] #
#################################################################################

## GOAL

- For each user, at each request (for now, no specific query, the user is asking at each step for a new object to check out), an object is suggested to the user
- Make fresh discoveries and maintaining high relevance

## CRITERION

- Compute a relevance & serendipity score for each object with respect to the considered user
- Maintaining a user network (that is, a set of similarly-behaving users, with respect to their activity -whether they have rated or not an object- and ratings of objects) and an object region (that is, a set of similar objects with respect to their features and a given distance)
- Score w.r.t. user object region stickiness (i.e. is the user already familiar with objects close to the considered object?) & user network (i.e. are people having the same behaviour as the considered user familiar with the considered object?)

## METHOD

- K-means++ to obtain the object regions (attribute-based similarity)
- Stickiness of user to region (~ number of ratings of objects in this region)
- Stickiness of user network to region (~ number of ratings by users of this network of objects in this region) => OTB-ness
- Final score: item relevance weighted by OTB-ness wrt user network stickiness and user stickiness

## EXPERIMENTS
- Measure of validation: discounted cumulative gain

###########################################
# "Serendipity: A New Recommender System" #
###########################################

## CONCEPTS

- Diversity: all of the items can be recommended, regardless of their popularity among the users (To be diverse avoids suggesting the same objects to all of the userbase).
- Serendipity

## METHOD

- Idea of regrouping objects in blobs -> object regions in [Abassi et al., 2009]
- Idea of relative percolation -> object stickiness in userbase [Abassi et al., 2009]
- Use of a serendipity threshold
- Cold start: initialize the distances to objects as in the "zero-knowledge approach"
- Otherwise: distances as "preference-based approach"

## INDICATORS/MEASURES OF VALIDATION

- Relative knowledge for a user of an object attribute (we are looking for broading the knowledge of the user, i.e. to make them enjoy new things)
- Relative percolation (we do not want the user to see the same "type of things")

###################################################
# "A Gang of Bandits" [Cesa-Bianchi et al., 2013] #
###################################################

"Using always the same system to perform recommendation may produce non stationary performance due to boredom (or more complex evolution) of the user interest. To fight that reinforcement learning techniques have been proposed to alternate between several systems at the right cadence. However, those solution learn the same policy for all users whereas we can expect that each user has her own boredom evolution. Leveraging gang of bandit algorithm could help to personalize the model to find how to alternate among different recommender system for each user." (extracted from the class project webpage)

#####################
# "Rotting Bandits" #
#####################

"In real world, reward for an action might not be constant. Rotting Bandit models the specific case where the value may (or may not) decay each time an action is taken. Such setup can model boredom in recommender system or cost increment due to repetition (e.g. scarcity of resource). Our recent work suggests that this problem is in fact as easy as classic stochastic bandit, as a simple but well designed strategy (FEWA [3] can recover similar regret bounds on the rotting bandit problem than UCB1 on the stochastic bandit problem [1]." (extracted from the class project webpage)


