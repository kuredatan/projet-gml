\documentclass{article}
\usepackage[final]{nips_2016}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Recommender system with serendipity}
\author{
  Cl\'{e}mence~R\'{e}da\\
  supervised by Claire~Vernade\\
}

\begin{document}

\nocite{*}

\maketitle

\begin{abstract}
A lacking feature of current recommender systems is that they usually do not allow discovery of new elements, that is, an element that might be interesting, but which is different of what the user is used to. Moreover, a clear definition of serendipity is missing. The goal of this project was (1) to formalize the problem of recommendation allowing serendipities, that is, surprisingly good discoveries; (2) to design a method which solves this problem in an online setting; (3) to evaluate its relevance with respect to the random strategy, usual collaborative filtering recommender system, and methods using Rotting Bandits.
\end{abstract}

\section{Introduction And Review of the State-of-the-Art}

%TODO

\section{Formalization of the Problem of Recommendation With Serendipity}

Serendipities are objects that are unexpectedly enjoyable. A more formal definition would be, given a serendipity threshold t, a similarity graph on the objects, and a given user to which recommendations would be made, serendipities are objects that are located in unexplored (with respect to the considered user) object regions (i.e. strongly connected components of the graph), at distance (in terms of length of the shortest path from explored regions to the considered unexplored region) greater than t, such that these objects can increase in expectation the diameter of the explored regions of the similarity graph by the user across time. In mathematical terms:

* G(V, E), undirected, unweighted similarity graph on objects
* u, a user 
* f-t-u : V -> {0, 1}, a function that indicates exploration of an object by user u (at time t)
* r-u : V -> [|0, 1, 2|], a function of rating by user u of objects (1 means "not seen", 0 means "don't like", 2 means "like")
* s, serendipity threshold
Then we define the set of serendipities at a given point in time t as:
S = {v in V : v = argmax-{v' in V, v' at distance min. s of an explored region of u} E(sum-{v" in V} [f-{t+1}-u(v")-f-{t}-u(v")]*r-u(v"))}
(objects which are not in explored regions and give the greatest increase in the diameter of explored regions of the graph, weighted by the known ratings given by the user)

where E is the expectation: exploration and rating of a node are drawn from a random distribution: when an object is recommended, we "make" the user explore the recommended node, and this modifies the exploration/rating distributions (if the user has enjoyed the recommended object, they will most likely be willing to explore the neighbours of this object).
f-u would likely be submodular (a node can be explored only once, thus at a given point in time, the number of explored elements will increase less and less strongly).

\section{Method}

The idea would be to use the OIMP (Online Influence Maximization with Persistence) method developped in [Lagrée et al., 2017]. The concept of restricting the set of candidates to a number K of objects (with their supports) might suit the idea to restrict recommendations to non-explored suitable regions:

For a given user u, a fixed parameter K (number of candidates)
For each round 1 up to a finite horizon T
	Compute the set of relevant candidates c-1, ..., c-k with their support A-1, ..., A-k (*)
	Compute a score of relevance for each candidate
	Select the (1) candidate with the highest score
	Observe reward r (rating of the recommended object by user u)
	Write the associated regret: real best candidate reward (among the k selected candidates) - r
	Improve the algorithm: update statistics accordingly for score computation
Return cumulative regret up to horizon T for comparison with other methods
In order to check the serendipity part, we can also keep track of the exploration of each object region: if the recommendation has succeeded in increasing the exploration of the user, the exploration of each region should increase: a mean to quantify this is to compute the volume of the parallelotope which relies on explored (by user u) objects o-1, o-2, ..., o-n as follows: V = (o-1, ..., o-n) and Vol(o-1, ..., o-n) = sqrt(det(V*V-T)) . This volume should increase more for recommendation with serendipity than for regular recommendation, because recommended objects are supposed to be less correlated (similar). [1]

The goal of this algorithm is then to infer the real edge weights (which corresponds, for an edge (i, j), to the probability of user u liking object i (j) knowing its rating of object j (i)) in an online setting.

(*) This is a difficult problem. Lagrée et al. suggest Max-degree or Cover as general strategies to find plausible candidates. One idea is to implement serendipity at this point of the bandit algorithm: select 1 "centroid" (either by K-means, or by Max-degree) in each unexplored object region at distance at least s from the explored regions.

Another issue is the missing data in most of the datasets (MovieLens included). We can restrict movies to objects having a minimal number of ratings, and, for each movie, the reward is a noisy observation of the empirical mean of the ratings in the dataset (using multinomial or Gaussian distributions): "semi-simulated data".

We implicitly assume that the number of "centroids" will be far lesser than the number of nodes in the object graph. When computing the regret, we also assume that the best "arm" we could use is among the K selected candidates, thus that we do not question the method with which we select the candidates, which is an hypothesis made in the [Lagrée et al., 2017] article.

\section{Experiments}

Comparison with Random strategy, if time permits, with Rotting Bandits and Linear UCB. Horizon T = 1,000. Comparison of the expected cumulative regret and evolution of volume up to horizon T.

%TODO Random, Rotting Bandits, Collaborative Filtering recommender

\section{Discussion}

%TODO

\subsubsection*{Acknowledgments}

Many thanks to Ms. Claire Vernade, who has supervised me, and has given useful advice and mentoring for this project.

\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}

%4-6 pages
%There is also a \verb+\paragraph+ command available, which sets the heading in bold, flush left, and inline with the text, with the heading followed by 1\,em of space.
%Of note is the command \verb+\citet+, which produces citations appropriate for use in inline text. 
%Footnotes should be used sparingly.  If you do require a footnote, indicate footnotes with a number\footnote{Sample of the first footnote.} in the text. Place the footnotes at the bottom of the page on which they appear. Note that footnotes are properly typeset \emph{after} punctuation
%\begin{figure}[h]
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%\end{figure}
%\begin{table}[t]
%  \caption{Sample table title}
%  \label{sample-table}
%  \centering
%  \begin{tabular}{lll}
%    \toprule
%    \multicolumn{2}{c}{Part}                   \\
%    \cmidrule{1-2}
%    Name     & Description     & Size ($\mu$m) \\
%    \midrule
%    Dendrite & Input terminal  & $\sim$100     \\
%    Axon     & Output terminal & $\sim$10      \\
%    Soma     & Cell body       & up to $10-6$  \\
%    \bottomrule
%  \end{tabular}
%\end{table}
